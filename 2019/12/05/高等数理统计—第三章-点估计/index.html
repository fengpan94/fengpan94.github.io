<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="欢迎指正。">
<meta property="og:type" content="article">
<meta property="og:title" content="高等数理统计—第三章 点估计">
<meta property="og:url" content="http://www.fengpan.xyz/2019/12/05/高等数理统计—第三章-点估计/index.html">
<meta property="og:site_name" content="风磐&#39;Blog">
<meta property="og:description" content="欢迎指正。">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-11-23T11:58:44.115Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="高等数理统计—第三章 点估计">
<meta name="twitter:description" content="欢迎指正。">
  <link rel="canonical" href="http://www.fengpan.xyz/2019/12/05/高等数理统计—第三章-点估计/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>高等数理统计—第三章 点估计 | 风磐'Blog</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">风磐'Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">谁无暴风劲雨时 守得云开见月明</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
  </ul>

    

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="http://www.fengpan.xyz/2019/12/05/高等数理统计—第三章-点估计/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Feng Pan">
      <meta itemprop="description" content="高等数理统计、高等概率论学习心得">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="风磐'Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">高等数理统计—第三章 点估计

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-12-05 16:21:13" itemprop="dateCreated datePublished" datetime="2019-12-05T16:21:13+08:00">2019-12-05</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-23 19:58:44" itemprop="dateModified" datetime="2019-11-23T19:58:44+08:00">2019-11-23</time>
              </span>
            
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>欢迎指正。</p><a id="more"></a>
<h1 id="第三章-点估计"><a href="#第三章-点估计" class="headerlink" title="第三章 点估计"></a>第三章 点估计</h1><p>通过样本$X=(X_1,…,X_n)$对总体的分布函数进行统计推断，通常有参数化与分参数方法，后者有专门的课程—非参数统计，我们这里目前只讲参数统计。<strong>参数统计推断通常有两个基本问题，即参数估计和假设检验</strong>。参数估计又分为点估计和区间估计，由于区间估计与假设检验联系密切，我们放在假设检验后面来讲。</p>
<p>本章主要从以下几方面来讲：相关基本概念（重点是优良性）；无偏估计及UMVUE、UMRUE；极大似然估计、矩估计、最小二乘估计、简略介绍同变估计、稳健估计。（结合两到三本教材的内容）</p>
<p>本章主要侧重于介绍方法，点估计的性质（效、渐进性质）我们放在第四章来讲。</p>
<h2 id="3-1-基本概念"><a href="#3-1-基本概念" class="headerlink" title="3.1 基本概念"></a>3.1 基本概念</h2><p>设参数$g(\theta),X$为样本，用统计量$T=T(X)$作为未知参数$g(\theta)$的“猜测”称为<strong>点估计</strong>，此时称参数$g(\theta)$为<strong>待估参数</strong>。但并不是任意一个统计量都可以作为参数的估计，需要有一个准则，“无偏性”就是一个普遍认可的准则（但并不是唯一准则，也不是非要不可的准则，比如就有有偏估计，这里是为了顺利引出我们接下来的内容）。</p>
<h2 id="3-2-无偏估计及UMVUE、UMRUE"><a href="#3-2-无偏估计及UMVUE、UMRUE" class="headerlink" title="3.2 无偏估计及UMVUE、UMRUE"></a>3.2 无偏估计及UMVUE、UMRUE</h2><h3 id="3-2-1无偏估计"><a href="#3-2-1无偏估计" class="headerlink" title="3.2.1无偏估计"></a>3.2.1无偏估计</h3><p><strong>定义3.1</strong> 若$g(\theta)$的估计为$\hat{g}(X)$,这里的$g(\theta)$为未知参数的函数，则称</p>
<p>​                                                         $E_\theta[\hat{g}(X)-g(\theta)]=bias[\hat{g}(X)]$</p>
<p>为$\hat{g}(X)$的偏差。若对一切$\theta,bias[\hat{g}(X)]=0$，则称$\hat{g}(X)$为$g(\theta)$的无偏估计（UE），即</p>
<p>​                                                              $E_\theta[\hat{g}(X)]=g(\theta),\forall\theta\in\Theta$ </p>
<p>关于无偏估计，我们有三点需要说明：</p>
<p>（1）无偏估计不一定存在</p>
<blockquote>
<p>我们接下来的讨论都是对可估参数而言，不可估的参数讨论其无偏估计是无意义的。</p>
</blockquote>
<p>（2）对可估参数，无偏估计一般不唯一</p>
<blockquote>
<p>我们将可估参数$g(\theta)$的无偏估计归为一类，记为$U_g$</p>
</blockquote>
<p>（3）无偏估计并不一定是一个好估计</p>
<hr>
<p>正如上面所说，无偏估计不唯一，那么我们就想在这堆无偏估计里面找一个最好的，什么样的最好？就需要我们有一个<strong>标准（即后面说的损失函数）</strong>。依据Wald于1950年提出的统计判决理论的观点，统计推断追求的<strong>目标</strong>是对于给定的<strong>损失函数</strong>$L(\theta,d)$，希望求出<strong>统计判决函数</strong>$\delta(x)$<strong>(即我们要求的统计量)</strong>，使其<strong>风险函数</strong>$R(\theta,\delta)$尽可能小。</p>
<p>$\bigstar\bigstar\bigstar$上面这句话总结了我们求一致最小风险\方差无偏估计的思路。比如在估计问题中，用统计量$T(X)$估计$g(\theta)$，损失函数(通常都要求为$T(X)$的凸函数)取$L(\theta,T(X))=(T(X)-g(\theta))^2$，则其相应的风险函数为$R(\theta,T(X))=E_\theta[L(\theta,T(X))]=E_\theta[T(X)-g(\theta)]^2$,此时的风险函数即为我们熟知的<strong>均方误差</strong>，当然也有其他的损失函数比如绝对损失。为使 风险函数\均方误差 最小，我们就可以求出相应的最小 风险\方差 估计量。</p>
<p>上面这一段我们说了要用统计量$T(X)$估计$g(\theta)$，那我们要用什么统计量，肯定不能是随便一个统计量吧，在此我们有这样一个定理</p>
<p><strong>定理3.1 （Rao-Blackwell）</strong>对于分布族$X\sim(\mathbb{X},\mathbb{B},P^X_\theta),\theta\in\Theta$，若$L(\theta,d)$为统计判决问题的凸损失函数，$T(X)$为<strong>充分统计量</strong>，$\delta(x)$为任一统计判决函数，则</p>
<p>​                                                                    $\delta^*(x)=E_\theta[\delta(X)|T=T(x)]$</p>
<p>必优于或等同于$\delta(x)$。  若$L(\theta,d)$为$d$的严凸函数，则$\delta^<em>(x)$一致优于$\delta(x)$，而$\delta^</em>(x)$等同于$\delta(x)$的充要条件为$\delta(x)$是$T(X)$的函数，即$\delta(x)=h(T(X))$。</p>
<blockquote>
<p>上面这个定理就是告诉我们，统计判决的最优解通常就是<strong>充分统计量的函数</strong>。</p>
</blockquote>
<h3 id="3-2-2-一致最小风险-方差无偏估计（UMRUE-UMVUE）"><a href="#3-2-2-一致最小风险-方差无偏估计（UMRUE-UMVUE）" class="headerlink" title="3.2.2 一致最小风险\方差无偏估计（UMRUE\UMVUE）"></a>3.2.2 一致最小风险\方差无偏估计（UMRUE\UMVUE）</h3><p>上一小节我们说要从无偏估计量里面找一个充分统计量使得风险最小，在这里给出定义。</p>
<p><strong>定义3.2</strong> 对于一般凸损失函数$L(\theta,d)$，若存在$g(\theta)$的无偏估计$\hat{g}(\theta)$，使得对任何其他无偏估计$ \tilde{g}(\theta)$有</p>
<p>$R(\theta,\hat{g}(\theta))\leq R(\theta, \tilde{g}(\theta)).\forall \theta\in\Theta$，则称$\hat{g}(\theta)$为$g(\theta)$的<strong>一致最小风险无偏估计（UMRUE）</strong>.</p>
<p>对于损失函数$L(\theta,d)=(d-g(\theta))^2$ ，此时的风险函数即为均方误差，若$Var_\theta[\hat{g}(X)]\leq Var_\theta[\tilde{g}(X)],\forall\theta\in\Theta$，则称$\hat{g}(\theta)$为$g(\theta)$的<strong>一致最小方差无偏估计（UMVUE）</strong>.</p>
<blockquote>
<p>到这里我们能看出来，<strong>均方误差是风险函数的一个特例</strong>。</p>
<p>在上一节我们给出的定理，最优解是充分统计量的函数，但仅仅如此是不够的。唯一性需要完备统计量来保证，最优性由完备统计量来保证，这里能出俩引理，就不列出了。直接给出最终版本的定理。</p>
</blockquote>
<p><strong>定理3.2 （Lehmann-Scheffe）</strong> 给定样本$X_1,…,X_n$，设$X=(X_1,…,X_n)’\sim f(x,\theta),\theta\in\Theta$。考虑$g(\theta)$的无偏估计，损失函数$L(\theta,d)$为凸函数，$T=T(X)$为<strong>完备的充分统计量</strong>，则有：</p>
<p>（1）设$\hat{g}(\theta)$为$g(\theta)$的无偏估计，且$\hat{g}(X)$为$T(X)$的函数，即$\hat{g}(X)=h(T(X))$，则$\hat{g}(X)$必为$g(\theta)$的一致最小风险无偏估计（UMRUE）。</p>
<p>（2）设$\tilde{g}(X)$为$g(\theta)$的无偏估计，则$\hat{g}(X)=E_\theta[\tilde{g}(X)|T]$为$g(\theta)$的一致最小风险无偏估计。</p>
<p>（3）若$L(\theta,d)$为严格凸，且$g(\theta)$的一致最小风险无偏估计存在，则必为$T=T(X)$的函数。</p>
<p>若损失函数为$L(\theta,d)=(d-g(\theta))^2$，上述(1)(2)(3)都成立，只不过不再叫一致最小风险无偏估计，称为一致最小方差无偏估计（UMRUE）。</p>
<h3 id="3-2-3-解题方法和一个例题"><a href="#3-2-3-解题方法和一个例题" class="headerlink" title="3.2.3 解题方法和一个例题"></a>3.2.3 解题方法和一个例题</h3><p>在完备充分统计量和一致最小风险无偏估计存在的前提下，我们根据Lehmann-Scheffe定理，有两种求解$g(\theta)$的UMRUE/UMVUE的方法：</p>
<p>（1）<strong>直接方法</strong>：找一个完备充分统计量$T(X)$的函数$\varphi(T)$使$E_\theta[\varphi(T(X))]=g(\theta)$，则$\hat{g}(X)=\varphi(T(X))$为$g(\theta)$的UMRUE。</p>
<p>（2）<strong>条件期望法</strong>：即取一个完备充分统计量$T(X)$以及$g(\theta)$的某一个无偏估计$\tilde{g}(X)$，则$\hat{g}(X)=E_\theta[\tilde{g}(X)|T]$为$g(\theta)$的UMRUE。这个方法关键在于求条件期望，较为麻烦。</p>
<p>下面给出一个例题，用以上两种方法解，但这并不意味着这两种方法<strong>都</strong>能同时去解其他问题，有时候根据分布、问题类型要选择其中一个，这就需要做大量的练习以熟练。</p>
<p><strong>例1</strong> 设$X_1,…,X_n$独立同分布，均服从区间$(0,\theta)(\theta&gt;0)$上的均匀分布，样本为$X=(X_1,X_2,…,X_n)$。求参数$g(\theta)=\theta^2$的UMVUE。</p>
<p>首先我们知道统计量$X_{(n)}=\max_{1\leq i\leq n}X_i$为一个完全充分统计量。</p>
<p><strong>直接方法：</strong>找一个合适的$X_{(n)}$的函数$\varphi(X_{(n)})$，使得$\varphi(X_{(n)})$成为$g(\theta)=\theta^2$的无偏估计，即</p>
<p>$E_\theta[\varphi(X_{(n)})]=\theta^2,\theta&gt;0$。为此，首先注意到$X_{(n)}$的概率密度函数为</p>
<script type="math/tex; mode=display">
p(t;\theta)=\begin{cases} 
\frac{nt^{n-1}}{\theta^n},  & 0<t<\theta \\
0, & 其他
\end{cases}</script><p>我们看一下它的期望</p>
<script type="math/tex; mode=display">
\begin{align}
E_\theta X_{(n)} & = \int p(t;\theta)\cdot t~dt \\
& = \int^\theta_0 \frac{nt^{n-1}}{\theta^n}\cdot t~dt\\
&= \int^\theta_0 \frac{nt^n}{\theta^n}~dt=\frac{n}{n+1}\theta\\
E_\theta X^2_{(n)} & =\int p(t;\theta)\cdot t^2~dt \\
& = \int^\theta_0 \frac{nt^{n-1}}{\theta^n}\cdot t^2~dt\\
&= \int^\theta_0 \frac{nt^{n+1}}{\theta^n}~dt=\frac{n}{n+2}\theta^2\\
\end{align}</script><p>于是我们就有$E_\theta[(\frac{n+2}{n})X^2_{(n)}]=\theta^2,\theta&gt;0$，就得到$(\frac{n+2}{n})X^2_{(n)}$是$\theta^2$的无偏估计，进而是UMVUE。</p>
<p><strong>条件期望法：</strong> 我们先找一个$\theta^2$的无偏估计，$E_\theta X^2_1=\int t^2\frac{1}{\theta}dt=\frac{\theta^2}{3}$，可见$3X^2_1$是$\theta^2$的一个无偏估计，进而条件数学期望$E_\theta[3X^2_1|X_{(n)}]$为$\theta^2$的UMVUE。接下来我们就算这个$E_\theta[3X^2_1|X_{(n)}]$，首先我们可以看出。当$X_{(n)}=t$时，$X_1$有$\frac{1}{n}$的概率取值为$t$，有$1-\frac{1}{n}$的概率服从区间$(0,t)$上的均匀分布。由此可得</p>
<script type="math/tex; mode=display">
\begin{align}
E_\theta[3X^2_1|X_{(n)}=t] & =(\frac{1}{n})\cdot3t^2+(1-\frac{1}{n})\int^t_0 3u^2 \frac{1}{t}du \\
& =\frac{3t^2}{n}+(\frac{n-1}{n})\frac{t^2}{3} \\
&=(\frac{n+2}{n})t^2 \\
\end{align}</script><p>可见$E_\theta[3X^2_1|X_{(n)}=t]=(\frac{n+2}{n})X^2_{(n)}$即是$\theta^2$的UMVUE。</p>
<h2 id="3-3-极大似然估计（MLE）"><a href="#3-3-极大似然估计（MLE）" class="headerlink" title="3.3 极大似然估计（MLE）"></a>3.3 极大似然估计（MLE）</h2><p>极大似然估计在直观上可以这样解释：使得出现所选样本最大概率的分布参数的估计。</p>
<p><strong>定义3.3</strong> 设$X\sim f(x;\theta),\theta\in\Theta$，把$f(x;\theta)$视为$\theta$的函数，则称它为$X$关于$\theta$的<strong>似然函数</strong>，$L(\theta,x)=\ln f(x;\theta)=L(\theta)$称为<strong>对数似然函数</strong>，若$\hat{\theta}(x)$满足</p>
<script type="math/tex; mode=display">
f(x,\hat{\theta}(x))=\max_{\theta\in\Theta}f(x;\theta)</script><p>则称$\hat{\theta}(x)$为$\theta$的<strong>极大似然估计(MLE)</strong>。</p>
<blockquote>
<p>若极大似然估计存在，则它必为充分统计量的函数。（借助因子分解定理）</p>
</blockquote>
<p>关于极大似然估计的计算流程这个在本科阶段就有，因此不再赘述。有关极大似然估计的性质，我们在第四章讲。</p>
<h2 id="3-4-矩估计"><a href="#3-4-矩估计" class="headerlink" title="3.4 矩估计"></a>3.4 矩估计</h2><p>矩估计也叫矩方程估计，是比较老的方法，其理论基础就是独立同分布情况下的大数定律，即观察值的样本平均趋向于总体平均。</p>
<p>设$X_1,…,X_n$独立同分布，$X_1\sim f(x_1;\theta),\theta\in\Theta$，有</p>
<p>总体原点矩：$\mu_j=E(X_1)^j$，其中$\mu_1=E(X_1)$</p>
<p>总体中心矩：$\alpha_j=E(X_1-\mu_1)^j$，其中$\alpha_2=E(X_1-\mu_1)^2=Var(X_1)=\sigma^2$</p>
<p>样本原点矩：$a_j=\frac{\sum^n_{i=1} X^j_i}{n}$，其中$a_1=\overline{X}$</p>
<p>样本中心矩：$m_j=\frac{\sum^n_{i=1} (X_i-\overline{X})^j}{n}$，其中$m_2=\frac{\sum^n_{i=1} (X_i-\overline{X})^2}{n}$</p>
<p>根据独立同分布随机变量序列的大数定律，当$n\rightarrow\infty$时，$a_j$以概率收敛（几乎处处收敛）到$\mu_j$；类似的有$m_j$以概率收敛（几乎处处收敛）到$\alpha_j$。因此可以自然得到</p>
<script type="math/tex; mode=display">
\hat{\mu}_j=a_j=\frac{\sum^n_{i=1} X^j_i}{n},~~~\hat{\alpha}_j=m_j=\frac{\sum^n_{i=1} (X_i-\overline{X})^j}{n} \notag{}</script><h2 id="3-5-最小二乘估计（LSE）"><a href="#3-5-最小二乘估计（LSE）" class="headerlink" title="3.5 最小二乘估计（LSE）"></a>3.5 最小二乘估计（LSE）</h2><p>最小二乘估计常用于线性模型求解，这个在很多的计量经济课本上都有介绍。</p>
<p>简单来说，在一个线性模型$Y=\alpha+\beta X+\epsilon$中，我们希望求得的系数（估计值）$\hat{\beta}$使得$(Y-\hat{\beta}X)^2$最小，一般是求(偏)导来求解，但如果$X$是满秩矩阵，我们可以通过$\hat{\beta}_{LS}=(X’X)^{-1}X’Y$快速计算出结果。</p>
<p>关于最小二乘的计算以及各种特殊情况（现实中较为常见）的处理，可以找一本<strong>线性模型</strong>的书来看。</p>
<h2 id="3-6-同变估计"><a href="#3-6-同变估计" class="headerlink" title="3.6 同变估计"></a>3.6 同变估计</h2><p>在一些参数估计问题中，要求寻找的估计量在样本作某种特定变换下保持某种统计性质。变换主要有三种：位移变换$X+c$、尺度变换$\alpha X$、线性变换$\alpha X+c$。于是就有这三种变换下的同变估计。同变估计，即在某种变换群下保持同变的估计。由于其严格的定义需由变换群和统计决策问题进行，因此我们在这仅给出同变估计的描述性定义。</p>
<p><strong>定义</strong> 设$\hat{\theta}(X_1,…,X_n)$是$\theta$的一个估计量，如果在样本作某种特定变换下，估计量$\hat{\theta}$具有某种相应的性质，则称$\hat{\theta}$是在该变换下$\theta$的<strong>同变估计</strong>。</p>
<blockquote>
<p>具体可见《参数统计教程》的第四章，韦博成老师专门用一章的篇幅来介绍。</p>
</blockquote>
<h2 id="3-7-稳健估计"><a href="#3-7-稳健估计" class="headerlink" title="3.7 稳健估计"></a>3.7 稳健估计</h2><p>连续性原理：如果一个方法在该模型下是最优的，则它应在该模型附近是几乎最优的。</p>
<p>具备这种连续性的方法称为<strong>稳健</strong>的。</p>
<p>稳健统计中一类常用的估计是M估计，它是Huber在1954年对极大似然估计加以引申而得出的。</p>
<p><strong>定义</strong> 设$X_1,…,X_n$是来自某总体的一个样本，$\rho(x;\theta)$为一选定的非负函数，若估计$\hat{\theta}=\hat{\theta}(X)$满足</p>
<script type="math/tex; mode=display">
\sum^n_{i=1}\rho(X_i;\hat{\theta})=\min_\theta \sum^n_{i=1}\rho(X_i;\theta)</script><p>则称$\hat{\theta}$为$\theta$的一个<strong>M估计</strong>。</p>
<p>若$\rho(x;\theta)$关于$\theta$可微，即$\phi(x;\theta)=\frac{\partial\rho(x;\theta)}{\partial\theta}$，如果$\hat{\theta}$满足</p>
<script type="math/tex; mode=display">
\sum^n_{i=1}\phi(X_i;\theta)=0</script><p>则$\hat{\theta}$也称为$\theta$的一个<strong>M估计</strong>。</p>
<p>上面两个公式的关系就像极大似然估计的定义和通过似然方程求解极大似然估计的关系一样。</p>

    </div>

    
    
    
        
      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/12/05/高等数理统计—第二章-完全充分统计量/" rel="next" title="高等数理统计—第二章 充分统计量、完备性、样本信息">
                  <i class="fa fa-chevron-left"></i> 高等数理统计—第二章 充分统计量、完备性、样本信息
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2019/12/05/高等数理统计—第四章-点估计的性质/" rel="prev" title="高等数理统计—第四章 点估计的性质">
                  高等数理统计—第四章 点估计的性质 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#第三章-点估计"><span class="nav-number">1.</span> <span class="nav-text">第三章 点估计</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-基本概念"><span class="nav-number">1.1.</span> <span class="nav-text">3.1 基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-无偏估计及UMVUE、UMRUE"><span class="nav-number">1.2.</span> <span class="nav-text">3.2 无偏估计及UMVUE、UMRUE</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1无偏估计"><span class="nav-number">1.2.1.</span> <span class="nav-text">3.2.1无偏估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-一致最小风险-方差无偏估计（UMRUE-UMVUE）"><span class="nav-number">1.2.2.</span> <span class="nav-text">3.2.2 一致最小风险\方差无偏估计（UMRUE\UMVUE）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-解题方法和一个例题"><span class="nav-number">1.2.3.</span> <span class="nav-text">3.2.3 解题方法和一个例题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-极大似然估计（MLE）"><span class="nav-number">1.3.</span> <span class="nav-text">3.3 极大似然估计（MLE）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-矩估计"><span class="nav-number">1.4.</span> <span class="nav-text">3.4 矩估计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-最小二乘估计（LSE）"><span class="nav-number">1.5.</span> <span class="nav-text">3.5 最小二乘估计（LSE）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-6-同变估计"><span class="nav-number">1.6.</span> <span class="nav-text">3.6 同变估计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-7-稳健估计"><span class="nav-number">1.7.</span> <span class="nav-text">3.7 稳健估计</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Feng Pan</p>
  <div class="site-description" itemprop="description">高等数理统计、高等概率论学习心得</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
  </nav>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Feng Pan</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.4.0</div>

        












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/muse.js?v=7.4.0"></script>
<script src="/js/next-boot.js?v=7.4.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
